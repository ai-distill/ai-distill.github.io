<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>[PyTorch_01]Basic Concepts of PyTorch</title>
      <link href="/2019/10/07/PyTorch-01-Basic-Concepts-of-PyTorch/"/>
      <url>/2019/10/07/PyTorch-01-Basic-Concepts-of-PyTorch/</url>
      
        <content type="html"><![CDATA[<h1 id="Tensor"><a href="#Tensor" class="headerlink" title="Tensor"></a>Tensor</h1><blockquote><p>Tensor in the PyTorch is the same as ndarray in numpy, and could be convert to each other. </p></blockquote><p><strong>create Tensor</strong></p><pre><code>int_tensor =torch.IntTensor([1, 2])long_tensor = torch.LongTensor([1, 2])short_tensor = torch.ShortTensor([1, 2])double_tensor = torch.DoubleTensor([1, 2])float_tensor = torch.FloatTensor([1, 2])</code></pre><p><strong>Default type of Tensor is FloatTensor, if you want to change their type, you can add your wanted type after the variable</strong></p><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># convert float into double in Tensor</span>d_torch_e <span class="token operator">=</span> torch_e<span class="token punctuation">.</span>double<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>d_torch_e<span class="token punctuation">)</span></code></pre><p><strong>about Tensor size</strong></p><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># convert float into double in Tensor</span>a <span class="token operator">=</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">9</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>a<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># returns the shape of a</span><span class="token keyword">print</span><span class="token punctuation">(</span>a<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 3</span><span class="token keyword">print</span><span class="token punctuation">(</span>a<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 2</span><span class="token keyword">print</span><span class="token punctuation">(</span>a<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># is equal to a.size(0)</span><span class="token keyword">print</span><span class="token punctuation">(</span>a<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># is equal to a.size(1)</span></code></pre><p><strong>normal distribution of tensor</strong></p><blockquote><p>return a standard normal distribution in Tensor</p><pre class=" language-python"><code class="language-python">d <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># # returns a matrix of 3 rows and 2 columns with a mean of 0 and a standard variance of 1.</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'normal random is : {}'</span><span class="token punctuation">.</span>format<span class="token punctuation">(</span>d<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>d<span class="token punctuation">)</span></code></pre></blockquote><p><strong>convert tensor into ndarray</strong></p><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># Assume variable a is a tensor</span>a <span class="token operator">=</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span>b <span class="token operator">=</span> a<span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre><p><strong>convert ndarray into tensor</strong></p><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># Assume variable a is a ndarray</span>a <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>b <span class="token operator">=</span> torch<span class="token punctuation">.</span>from_numpy<span class="token punctuation">(</span>a<span class="token punctuation">)</span></code></pre><p><strong>But ndarray in numpy is only supported in CPU. Tensor is supported between CPU and GPU</strong></p><pre class=" language-python"><code class="language-python">a <span class="token operator">=</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token keyword">if</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>is_available<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    a_cuda <span class="token operator">=</span> a<span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span>a_cuda<span class="token punctuation">)</span></code></pre><h1 id="Variable"><a href="#Variable" class="headerlink" title="Variable"></a>Variable</h1><blockquote><p>You can consider Variable as a computational graph. Variable is a class in torch.autograd, it provides many features, for example, automatic derivation.<br>Every Variable element has three main attributes : data(get the value of Variable(considered as a tensor wrapped with many functions)), grad(get backward grad), grad_fn(tell Variable how to get grad)</p></blockquote><p><strong>convert tensor into Variable(requires_grad parameter tells Variable if automatic derivation)</strong></p><pre><code>a = torch.Tensor([1, 2])a = torch.autograd.Variable(a, requires_grad=True)</code></pre><p><strong>Build a computational graph</strong></p><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># Build a computational graph</span>y <span class="token operator">=</span> w <span class="token operator">*</span> x <span class="token operator">+</span> b<span class="token comment" spellcheck="true"># Compute gradients</span>y<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># automatic derivation</span><span class="token comment" spellcheck="true"># Print out the gradients</span><span class="token keyword">print</span><span class="token punctuation">(</span>x<span class="token punctuation">.</span>grad<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>w<span class="token punctuation">.</span>grad<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>b<span class="token punctuation">.</span>grad<span class="token punctuation">)</span></code></pre><p><strong>Summary</strong></p><pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token comment" spellcheck="true"># Variable 变量，</span><span class="token comment" spellcheck="true"># 是神经网络图里的一个特有概念，提供了自动求导的功能，</span><span class="token comment" spellcheck="true"># 不过Variable会放入计算图中，进行前向传播和后向传播，自动求导</span><span class="token comment" spellcheck="true"># Variable 有3个重要的组成属性：data, grad_fn, grad</span><span class="token comment" spellcheck="true"># data 是取得Variable的数值</span><span class="token comment" spellcheck="true"># grad_fn 是得到这个Variable的操作，例如是通过加减还是乘除来得到这个Variable</span><span class="token comment" spellcheck="true"># grad 是这个Variable的反向传播梯度</span><span class="token comment" spellcheck="true"># x是一个tensor, 转换为variable x=Variable(x)就可以转换为Variable</span><span class="token comment" spellcheck="true"># 1. The difference between torch.tensor and torch.Tensor in the following sentences.</span><span class="token comment" spellcheck="true"># 构建Variable. 要记得传入一个参数 requires_grad=True，这个参数表示是否 对这个变量求梯度，默认的 是 Fa!se</span>x <span class="token operator">=</span> torch<span class="token punctuation">.</span>autograd<span class="token punctuation">.</span>Variable<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>w <span class="token operator">=</span> torch<span class="token punctuation">.</span>autograd<span class="token punctuation">.</span>Variable<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>b <span class="token operator">=</span> torch<span class="token punctuation">.</span>autograd<span class="token punctuation">.</span>Variable<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># Build a computational graph</span>y <span class="token operator">=</span> w <span class="token operator">*</span> x <span class="token operator">+</span> b<span class="token comment" spellcheck="true"># Compute gradients</span>y<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 自动求导</span><span class="token comment" spellcheck="true"># Print out the gradients</span><span class="token keyword">print</span><span class="token punctuation">(</span>x<span class="token punctuation">.</span>grad<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>w<span class="token punctuation">.</span>grad<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>b<span class="token punctuation">.</span>grad<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 上面是标量的求导，同时也可以做矩阵求导</span>x <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 返回一个张量，包含了从标准正态分布(mean=0, std=1)中抽取一组随机数，形状由可变参数sizes定义。</span><span class="token keyword">print</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span>x <span class="token operator">=</span> torch<span class="token punctuation">.</span>autograd<span class="token punctuation">.</span>Variable<span class="token punctuation">(</span>x<span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>y <span class="token operator">=</span> x <span class="token operator">*</span> <span class="token number">2</span><span class="token keyword">print</span><span class="token punctuation">(</span>y<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 这里对向量求导不能直接写成y.backward(), 这样程序会报错</span><span class="token comment" spellcheck="true"># 这个时候需要传入参数声明，比如y.backward(torch.FloatTensor([1, 0.1, 0.01]))</span><span class="token comment" spellcheck="true"># 得到的结果就是每个分量的梯度 分别 乘上1，0.1.0.01</span><span class="token comment" spellcheck="true"># 写成y.backward(torch.FloatTensor([1, 1, 1])) 就是每个分量的梯度</span>y<span class="token punctuation">.</span>backward<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>FloatTensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0.1</span><span class="token punctuation">,</span> <span class="token number">0.01</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>x<span class="token punctuation">.</span>grad<span class="token punctuation">)</span></code></pre><h1 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h1><blockquote><p>How to load data efficiently ?</p></blockquote><pre class=" language-python"><code class="language-python">train_data <span class="token operator">=</span> torchvision<span class="token punctuation">.</span>datasets<span class="token punctuation">.</span>MNIST<span class="token punctuation">(</span>    root<span class="token operator">=</span><span class="token string">'./mnist'</span><span class="token punctuation">,</span>    train<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>     <span class="token comment" spellcheck="true"># 设置是否为训练集数据，如果为True将会下载训练集，否则将会下载测试集数据</span>    transform<span class="token operator">=</span>torchvision<span class="token punctuation">.</span>transforms<span class="token punctuation">.</span>ToTensor<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>      <span class="token comment" spellcheck="true"># 把下载的数据改造成什么形式,原始数据是ndarray,改成一个Tensor数据</span>    download<span class="token operator">=</span>DOWNLOAD_MNIST<span class="token punctuation">)</span>data_loader <span class="token operator">=</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data<span class="token punctuation">.</span>DataLoader<span class="token punctuation">(</span>train_data<span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">32</span><span class="token punctuation">,</span> shuffle<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> collate_fn<span class="token operator">=</span>default_collate<span class="token punctuation">)</span></code></pre><p><strong>Summary</strong></p><pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token keyword">import</span> pandas <span class="token keyword">as</span> pd<span class="token comment" spellcheck="true"># Dataset 数据集</span><span class="token comment" spellcheck="true"># torch.utils.data.Dataset 是一个抽象类，因此可以自定义自己的数据读取类</span><span class="token comment" spellcheck="true"># 只需要定义__len__和__getitem__这两个函数</span><span class="token keyword">class</span> <span class="token class-name">myDataset</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data<span class="token punctuation">.</span>Dataset<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> csv_file<span class="token punctuation">,</span> txt_file<span class="token punctuation">,</span> root_dir<span class="token punctuation">,</span> other_file<span class="token punctuation">)</span><span class="token punctuation">:</span>        self<span class="token punctuation">.</span>csv_data <span class="token operator">=</span> pd<span class="token punctuation">.</span>read_csv<span class="token punctuation">(</span>csv_file<span class="token punctuation">)</span>        <span class="token keyword">with</span> open<span class="token punctuation">(</span>txt_file<span class="token punctuation">,</span> <span class="token string">'r'</span><span class="token punctuation">)</span> <span class="token keyword">as</span> f<span class="token punctuation">:</span>            data_list <span class="token operator">=</span> f<span class="token punctuation">.</span>readlines<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>txt_data <span class="token operator">=</span> data_list        self<span class="token punctuation">.</span>root_dir <span class="token operator">=</span> root_dir    <span class="token keyword">def</span> <span class="token function">__len__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> len<span class="token punctuation">(</span>self<span class="token punctuation">.</span>csv_data<span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">__getitem__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> idx<span class="token punctuation">)</span><span class="token punctuation">:</span>        data <span class="token operator">=</span> <span class="token punctuation">(</span>self<span class="token punctuation">.</span>csv_data<span class="token punctuation">[</span>idx<span class="token punctuation">]</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>txt_data<span class="token punctuation">[</span>idx<span class="token punctuation">]</span><span class="token punctuation">)</span>        <span class="token keyword">return</span> data<span class="token comment" spellcheck="true"># 虽然上面的方式可以定义我们需要的数据类型，但是很难实现batch_size, shuffle或多线程读取数据</span><span class="token comment" spellcheck="true"># 所以PyTorch提供了一个简单的方法来做这件事情，torch.utils.data.DataLoader来定义迭代器</span><span class="token comment" spellcheck="true"># collate_fn 表示如何取样本，可以定义自己函数来准确实现功能，但是通常默认参数就可以满足</span>dataiter <span class="token operator">=</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data<span class="token punctuation">.</span>DataLoader<span class="token punctuation">(</span>myDataset<span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">32</span><span class="token punctuation">,</span> shuffle<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> collate_fn<span class="token operator">=</span>default_collate<span class="token punctuation">)</span></code></pre><h1 id="How-to-save-model-and-load"><a href="#How-to-save-model-and-load" class="headerlink" title="How to save model and load"></a>How to save model and load</h1><blockquote><p>If your model have learned completely and you want to use in the next time, you should save the whole model (including the structure and parameters ) or save the parameters of model.<br>Maybe model has been saved completely, you should load it and then you can let it continue to work.</p></blockquote><h2 id="Save-the-whole-model"><a href="#Save-the-whole-model" class="headerlink" title="Save the whole model"></a>Save the whole model</h2><pre class=" language-python"><code class="language-python">torch<span class="token punctuation">.</span>save<span class="token punctuation">(</span>model<span class="token punctuation">,</span> <span class="token string">'./model.pth'</span><span class="token punctuation">)</span></code></pre><h2 id="Save-the-parameters-of-model"><a href="#Save-the-parameters-of-model" class="headerlink" title="Save the parameters of model"></a>Save the parameters of model</h2><pre class=" language-python"><code class="language-python">torch<span class="token punctuation">.</span>save<span class="token punctuation">(</span>model<span class="token punctuation">.</span>state_dict<span class="token punctuation">,</span> <span class="token string">'./model_state.pth'</span><span class="token punctuation">)</span></code></pre><h2 id="Load-the-whole-model"><a href="#Load-the-whole-model" class="headerlink" title="Load the whole model"></a>Load the whole model</h2><h2 id="Load-the-parameters-of-model"><a href="#Load-the-parameters-of-model" class="headerlink" title="Load the parameters of model"></a>Load the parameters of model</h2><p><strong>Note: before loading the parameters of model, you need to constructure this model</strong></p><pre class=" language-python"><code class="language-python">net <span class="token operator">=</span> LeNet5<span class="token punctuation">(</span><span class="token punctuation">)</span>net<span class="token punctuation">.</span>load_state_dict<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>load<span class="token punctuation">(</span><span class="token string">'model_state.pth'</span><span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre>]]></content>
      
      
      <categories>
          
          <category> PyTorch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DL </tag>
            
            <tag> AI </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>What is AutoEncoder?(Morvan)</title>
      <link href="/2019/09/30/What-is-AutoEncoder-Morvan/"/>
      <url>/2019/09/30/What-is-AutoEncoder-Morvan/</url>
      
        <content type="html"><![CDATA[<h2 id="What-is-AutoEncoder"><a href="#What-is-AutoEncoder" class="headerlink" title="What is AutoEncoder?"></a>What is AutoEncoder?</h2><p>AutoEncoder is an unsupervised learning method. It consists of a encoding and decoding action. Encoding is to compress data, and decoding is to decompress the data which be encoded.<br>Because the scale of NN is large, we often take some measurements to get thinner.</p>]]></content>
      
      
      <categories>
          
          <category> Deep Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DL </tag>
            
            <tag> AI </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
